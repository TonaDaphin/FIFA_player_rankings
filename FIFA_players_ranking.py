# -*- coding: utf-8 -*-
"""assignment sports prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eFFzA__cQTMJZAW_xwTCb8hQBDwp-x8A

### Data Collection
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np

# Load the dataset
data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/AI/Colab Notebooks/FIFA/players_22.csv')

data.head()

data.shape

"""### Data preparation and feature extraction"""

data.columns.values

data.loc[0:30,'club_name']

# Drop irrelevant columns
data = data.drop(['sofifa_id', 'player_url', 'real_face', 
                  'player_face_url', 'club_logo_url', 'club_flag_url', 'nation_logo_url', 'club_contract_valid_until','nationality_id',
                  'nation_team_id', 'league_level',
                  'nation_flag_url', 'nation_jersey_number', 'preferred_foot','weak_foot', 'body_type', 'dob', 'club_loaned_from', 
                  'club_joined','long_name', 'short_name', 'club_jersey_number', 'club_team_id' ], axis=1)

data.columns.values

#data.loc[0:30,'']

#data['mentality_mean'] = data[['mentality_aggression', 'mentality_interceptions', 'mentality_positioning', 'mentality_vision', 'mentality_penalties', 'mentality_composure']].mean(axis=1)

#see which columns have missing values
data.columns[data.isna().any()].tolist()

#see if any duplicated values are there
data.duplicated().sum()

"""###### Spliting numerical and categorical columns to impute missing data"""

# Split data into numeric and categorical columns
numeric_cols = data.select_dtypes(include=np.number).columns.tolist()
y = data['overall']
numeric_cols.remove('overall')
categorical_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()

# Impute missing values for numerical columns
data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].mean())

# impute missing values in categorical columns with the most frequent value
data[categorical_cols] = data[categorical_cols].fillna(data[categorical_cols].mode().iloc[0])

numeric_cols_missing = data[numeric_cols].isna().sum()
print(numeric_cols_missing)

categorical_cols_missing = data[categorical_cols].isna().sum()
print(categorical_cols_missing)

print(data.isnull().sum().any())

data.columns

from sklearn.preprocessing import OneHotEncoder, StandardScaler

data.columns

"""### Encoding and scaling the data"""

from sklearn.preprocessing import StandardScaler

# One-hot encode the categorical columns using pd.get_dummies()
data_encoded = pd.get_dummies(data, columns=categorical_cols)

# Scale the numerical columns using StandardScaler
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data_encoded[numeric_cols])
data_scaled = pd.DataFrame(data_scaled, columns=numeric_cols)

# Combine the encoded and scaled dataframes
data_processed = pd.concat([data_encoded.drop(columns=numeric_cols), data_scaled], axis=1)

# Calculate the correlation between each feature and the target variable
corr_matrix = data_processed.corrwith(data['overall'])

# Set a threshold for selecting important features
corr_threshold = 0.4

# Select the important features based on the correlation threshold
important_features = corr_matrix[abs(corr_matrix) > corr_threshold].index.tolist()

# Print the list of important features
print(len(important_features))

print(important_features)

# Drop the columns that are not in the important_features list
data_processed = data_processed[important_features]

data_processed.columns

"""### Training an XGBoost model for prediction"""

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data_processed, y, test_size=0.2, random_state=42)

# Define the XGBoost model
model = xgb.XGBRegressor(n_estimators=50, max_depth=6, learning_rate=0.1, objective='reg:squarederror')

# Train the model on the training data
model.fit(X_train, y_train)

# Use the trained model to make predictions on the test data
y_pred = model.predict(X_test)

# Calculate the mean absolute error (MAE) of the predictions
mae = mean_absolute_error(y_test, y_pred)
print('MAE:', mae)

#also do rsme
rmse = mean_squared_error(y_test, y_pred, squared=False)
print('RMSE:', rmse)

"""###### Optimising the model"""

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data_processed, y, test_size=0.2, random_state=42)

# Define the XGBoost model
model = xgb.XGBRegressor(n_estimators=20, max_depth=8, learning_rate=0.1, objective='reg:squarederror')

# Train the model on the training data
model.fit(X_train, y_train)

# Use the trained model to make predictions on the test data
y_pred = model.predict(X_test)

# Calculate the mean absolute error (MAE) of the predictions
mae = mean_absolute_error(y_test, y_pred)
print('MAE:', mae)

#also do rsme
rmse = mean_squared_error(y_test, y_pred, squared=False)
print('RMSE:', rmse)

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data_processed, y, test_size=0.2, random_state=42)

# Define the XGBoost model
model = xgb.XGBRegressor(n_estimators=80, max_depth=3, learning_rate=0.1, objective='reg:squarederror')

# Train the model on the training data
model.fit(X_train, y_train)

# Use the trained model to make predictions on the test data
y_pred = model.predict(X_test)

# Calculate the mean absolute error (MAE) of the predictions
mae = mean_absolute_error(y_test, y_pred)
print('MAE:', mae)

#also do rsme
rmse = mean_squared_error(y_test, y_pred, squared=False)
print('RMSE:', rmse)

"""

```
# This is formatted as code
```

### Testing the model on a new dataset"""

def predicting(data_path):

  # Drop irrelevant columns
  new_data = data_path.drop(['sofifa_id', 'player_url', 'real_face', 
                  'player_face_url', 'club_logo_url', 'club_flag_url', 'nation_logo_url', 'club_contract_valid_until','nationality_id',
                  'nation_team_id', 'league_level',
                  'nation_flag_url', 'nation_jersey_number', 'preferred_foot','weak_foot', 'body_type', 'dob', 'club_loaned_from', 
                  'club_joined','long_name', 'club_jersey_number', 'club_team_id' ], axis=1)

  # Impute missing values for numerical columns
  new_data[numeric_cols] = new_data[numeric_cols].fillna(new_data[numeric_cols].mean())

  # impute missing values in categorical columns with the most frequent value
  new_data[categorical_cols] = new_data[categorical_cols].fillna(new_data[categorical_cols].mode().iloc[0])

  # One-hot encode the categorical columns using pd.get_dummies()
  new_data_encoded = pd.get_dummies(new_data, columns=categorical_cols)

  # Scale the numerical columns using StandardScaler
  new_data_scaled = scaler.transform(new_data_encoded[numeric_cols])
  new_data_scaled = pd.DataFrame(new_data_scaled, columns=numeric_cols)


  # Combine the encoded and scaled dataframes
  new_data_processed = pd.concat([new_data_encoded.drop(columns=numeric_cols), new_data_scaled], axis=1)

  # Calculate the correlation between each feature and the target variable
  corr_matrix = new_data_processed.corrwith(data['overall'])

  # Set a threshold for selecting important features
  corr_threshold = 0.4

  # Select the important features based on the correlation threshold
  important_features = corr_matrix[abs(corr_matrix) > corr_threshold].index.tolist()

  # Print the list of important features
  #print(len(important_features))
  #print(important_features)


  # Drop the columns that are not in the important_features list
  new_data_processed = new_data_processed[important_features]

  model.fit(X_train, y_train)

  # Use the trained model to make predictions on the new data
  y_pred = model.predict(new_data_processed)

  # Print the predicted values
  #print(y_pred)

  from sklearn.metrics import mean_absolute_error
  y_test=new_data_processed.overall

  # Calculate the mean absolute error (MAE) of the predictions
  mae = mean_absolute_error(y_test, y_pred)

  #also do rsme
  rmse = mean_squared_error(y_test, y_pred, squared=False)

  return mae,rmse



# Load in a new dataset
path = pd.read_csv('/content/drive/My Drive/ai_work/fifa_folder/players_21.csv')
predicting(path)

import pickle
pickle.dump(model, open('model.pkl', 'wb'))

!pip install streamlit

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import pandas as pd
# import numpy as np
# from sklearn.preprocessing import OneHotEncoder, StandardScaler
# from sklearn.preprocessing import StandardScaler
# import xgboost as xgb
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import mean_absolute_error
# from sklearn.metrics import mean_squared_error
# import streamlit as st
# import pickle
# 
# def predicting(data_path):
# 
#   # Drop irrelevant columns
#   new_data = data_path.drop(['sofifa_id', 'player_url', 'real_face', 
#                     'player_face_url', 'club_logo_url', 'club_flag_url', 'nation_logo_url', 'club_contract_valid_until','nationality_id',
#                     'nation_team_id', 'league_level',
#                     'nation_flag_url', 'nation_jersey_number', 'preferred_foot','weak_foot', 'body_type', 'dob', 'club_loaned_from', 
#                     'club_joined','long_name', 'club_jersey_number', 'club_team_id' ], axis=1)
#   
#   # Split data into numeric and categorical columns
#   numeric_cols = new_data.select_dtypes(include=np.number).columns.tolist()
#   y = new_data['overall']
#   numeric_cols.remove('overall')
#   categorical_cols = new_data.select_dtypes(include=['object', 'category']).columns.tolist()
# 
#   # Impute missing values for numerical columns
#   new_data[numeric_cols] = new_data[numeric_cols].fillna(new_data[numeric_cols].mean())
# 
#   # impute missing values in categorical columns with the most frequent value
#   new_data[categorical_cols] = new_data[categorical_cols].fillna(new_data[categorical_cols].mode().iloc[0])
# 
#   # One-hot encode the categorical columns using pd.get_dummies()
#   new_data_encoded = pd.get_dummies(new_data, columns=categorical_cols)
# 
#   # Scale the numerical columns using StandardScaler
#   scaler = StandardScaler()
#   new_data_scaled = scaler.fit_transform(new_data_encoded[numeric_cols])
#   new_data_scaled = pd.DataFrame(new_data_scaled, columns=numeric_cols)
# 
# 
#   # Combine the encoded and scaled dataframes
#   new_data_processed = pd.concat([new_data_encoded.drop(columns=numeric_cols), new_data_scaled], axis=1)
# 
#   # Calculate the correlation between each feature and the target variable
#   corr_matrix = new_data_processed.corrwith(new_data['overall'])
# 
#   # Set a threshold for selecting important features
#   corr_threshold = 0.4
# 
#   # Select the important features based on the correlation threshold
#   important_features = corr_matrix[abs(corr_matrix) > corr_threshold].index.tolist()
# 
#   # Print the list of important features
#   #print(len(important_features))
#   #print(important_features)
# 
# 
#   # Drop the columns that are not in the important_features list
#   new_data_processed = new_data_processed[important_features]
# 
#   # ----------------------------------
#   # Define the XGBoost model
#   model = xgb.XGBRegressor(n_estimators=50, max_depth=6, learning_rate=0.1, objective='reg:squarederror')
# 
#   # Split the data into training and testing sets
#   X_train, X_test, y_train, y_test = train_test_split(new_data_processed, y, test_size=0.2, random_state=42)
# 
#   model.fit(X_train, y_train)
#   #----------------------
# 
#   
# 
#   # Use the trained model to make predictions on the new data
#   y_pred = model.predict(new_data_processed)
# 
#   # Print the predicted values
#   #print(y_pred)
# 
#   from sklearn.metrics import mean_absolute_error
#   y_test=new_data_processed.overall
# 
#   # Calculate the mean absolute error (MAE) of the predictions
#   mae = mean_absolute_error(y_test, y_pred)
# 
#   #also do rsme
#   rmse = mean_squared_error(y_test, y_pred, squared=False)
# 
#   return 'MAE',mae,'RMSE',rmse
# 
# 
# # Deployment
# 
# 
# st.title('Predict overall rating')
# 
# # Load in a new dataset
# path = pd.read_csv('/content/drive/My Drive/ai_work/fifa_folder/players_21.csv')
# 
# 
# prediction_var = ''
# 
# if st.button('Predict'):
#   prediction_var = predicting(path)
# 
# st.success(prediction_var)

!streamlit run app.py & npx localtunnel --port 8501

